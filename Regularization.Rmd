---
title: "Regularization"
output: 
  html_document: 
    theme: readable
    css: ../css/custom.css
---

```{r setup, include=FALSE}
library(glmnet)
library(ggplot2)
library(ISLR)
data(Hitters)
knitr::opts_chunk$set(echo = TRUE)
set.seed(10108)
```

********************************************************************************

## Overview

Demonstrate various regularization techniques used in regression analysis.  

##### Ordinary Least Squares
OLS regression fits a linear model with coefficients $w = (w_1, ..., w_p)$ to minimize the RSS between the observed and the predicted responses.  

Mathematically, it tries to minimize the objective function:  
$$
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\underset{w}{min\,} \norm {X w - y}_2^2
$$

##### Ridge Regression
Ridge regression imposes a penalty on the size of coefficients. Here, $\alpha \geq 0$ is a complexity parameter that controls the amount of shrinkage of coefficients.  

Objective to minimize:  
$$
\underset{w}{min\,} {\norm {X w - y}_2^2 + \alpha\, \norm w_2^2}
$$

##### Lasso Regression
Mathematically, it consists of a linear model trained with $\ell_1$ prior as regularizer. The lasso estimate solves the minimization of the least-squares penalty with $\alpha ||w||_1$ added, where $\alpha$ is a constant and $||w||_1$ is the $\ell_1$-norm of the parameter vector.  

Objective to minimize:
$$
\underset{w}{min\,} { \frac{1}{2n_{samples}} \norm {X w - y}_2 ^ 2 + \alpha\, \norm w_1}
$$

##### Elastic Net Regression
ElasticNet is a linear regression model trained with $\ell_1$ and $\ell_2$ prior as regularizer. We control the convex combination of $\ell_1$ and $\ell_2$ using the $\ell_1$-ratio ($\rho$) parameter.

Objective to minimize:  
$$
\underset{w}{min\,} { \frac{1}{2n_{samples}} \norm {X w - y}_2^2 + \alpha \rho \norm w_1 +\frac{\alpha(1-\rho)}{2} \norm w_2^2}
$$

## Dataset

Major League Baseball Data from the 1986 and 1987 seasons.

```{r descdata}
model.data <- Hitters[complete.cases(Hitters),]
str(model.data)
```

## Fitting the Models

We will use the R `glmnet` package for regressing the _Salary_ of the baseball players. In `glmnet`, the penalty on the coefficient vector is defined as
$$
\frac{1-\alpha}{2} \norm {\beta_j}_2^2 + \alpha\, \norm {\beta_j}_1,
$$
where $\alpha=1$ is the lasso penalty, and $\alpha=0$ the ridge penalty. For $0 < \alpha < 1$ you get the elastic net model.

```{r createmodel}
x <- model.matrix(Salary ~., model.data)[, -1]
y <- model.data$Salary
grid <- 10^seq(0.1, 5.25, length = 100)

ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
cv.ridge <- cv.glmnet(x, y, alpha = 0)

lasso.mod <- glmnet(x, y, alpha = 1, lambda = grid)
cv.lasso <- cv.glmnet(x, y, alpha = 1)

elastic.mod <- glmnet(x, y, alpha = 0.05, lambda = grid)
cv.elastic <- cv.glmnet(x, y, alpha = 0.05)
```

```{r coef}
ridge.mod$lambda[50]
```

Here are the coefficients when $\lambda = `r ridge.mod$lambda[50]`$ for ridge regression:
```{r coef2}
coef(ridge.mod)[,50]
```

To predict ridge regression coefficients for a new value of $\lambda$, say 50:
```{r coef3}
coef(ridge.mod, s = 50)[1:20,]
```

Minimum $\lambda$ and mean CV error for ridge regression:
```{r mincv}
c(cv.ridge$lambda.min, min(cv.ridge$cvm))
```

```{r modelselect, echo=FALSE, fig.width=9}
par(mfrow = c(1, 3), mar = c(5, 5, 5.5, 2), oma = c(0, 0, 1.5, 0))
plot(cv.ridge, main = "Ridge")
plot(cv.lasso, main = "Lasso")
plot(cv.elastic, main = "Elastic Net")
mtext("Model Selection", outer = T)
```

```{r coefestimates, echo=FALSE, fig.width=9}
par(mfrow = c(1, 3), mar = c(5, 5, 5.5, 2), oma = c(0, 0, 1.5, 0))
plot(ridge.mod, xvar = "lambda", main = "Ridge")
plot(lasso.mod, xvar = "lambda", main = "Lasso")
plot(elastic.mod, xvar = "lambda", main = "Elastic Net")
mtext("Coefficient Shrinkage", outer = T)
```



