---
title: "Regression"
output: 
  html_document: 
    theme: readable
    css: ../css/custom.css
---

```{r setup, include=FALSE}
library(ggplot2)
library(GGally)
library(corrplot)
data(mtcars)
knitr::opts_chunk$set(echo = TRUE)
set.seed(10108)
```

---


## Dataset

A data frame with 32 observations on 11 variables.

| var  | Description                              |
|------|------------------------------------------|
| mpg  | Miles/(US) gallon                        |
| cyl  | Number of cylinders                      |
| disp | Displacement (cu.in.)                    |
| hp   | Gross horsepower                         |
| drat | Rear axle ratio                          |
| wt   | Weight (1000 lbs)                        |
| qsec | 1/4 mile time                            |
| vs   | V/S                                      |
| am   | Transmission (0 = automatic, 1 = manual) |
| gear | Number of forward gears                  |
| carb | Number of carburetors                    |

```{r mtcars}
head(mtcars)
```

```{r mtcarsplot, fig.width=4, fig.height=4, echo=FALSE}
corrplot(cor(mtcars[, -1]), method="ellipse")
```
<br/>


## OLS Regression

Given a data set ${\{y_{i},\,x_{i1},\ldots ,x_{ip}\}_{i=1}^{n}}$ of n statistical units, the model takes the form:
$$
y_{i}=\beta _{1}x_{i1}+\cdots +\beta _{p}x_{ip}+\varepsilon _{i}=\mathbf {x} _{i}^{\rm {T}}{\boldsymbol {\beta }}+\varepsilon _{i},\qquad i=1,\ldots ,n,
$$
where,  
$y_{i}$ is the response variable,  
$x_{i1},\,x_{i2},\,\ldots ,\,x_{ip}$ are the regressors,  
$\beta$ is a p-dimensional parameter vector (estimated by least-squares method)  
$\epsilon_{i}$ is the error term.  

### Fitting the Model

We will regress the *mpg* variable onto *disp, cyl, am*. A linear model is fitted by least-squares, which minimizes the sum of squared residuals and  leads to a closed-form expression for the estimated value of the unknown parameter $\boldsymbol \beta$.  

Objective:
$$
{\hat {\boldsymbol {\beta }}}={\rm {arg}}\min _{\boldsymbol \beta }\,\left\lVert \mathbf {y}-\mathbf {X}{\boldsymbol \beta } \right\rVert ,
$$

which leads to:
$$
{\hat {\boldsymbol {\beta }}}=(\mathbf {X} ^{\rm {T}}\mathbf {X} )^{-1}\mathbf {X} ^{\rm {T}}\mathbf {y} ={\big (}\,{\textstyle \sum }\mathbf {x} _{i}\mathbf {x} _{i}^{\rm {T}}\,{\big )}^{-1}{\big (}\,{\textstyle \sum }\mathbf {x} _{i}y_{i}\,{\big )}.
$$

```{r createmodel}
model <- lm(mpg ~ cyl + disp + am, data = mtcars)
model$coefficients
```

```{r summary, echo=FALSE}
model.summ <- summary(model)
```

Adjusted R-squared: `r model.summ$adj.r.squared`  
F-statistic: `r model.summ$fstatistic[1]`

```{r residualplot, echo=FALSE, fig.width=5, fig.height=4}
source("lmplots.R")
diagPlot(model)$rvfPlot
```

```{r predictsales}
# sales vs predicted sales
head(data.frame(mpg = mtcars$mpg, pred = predict(model)), 10)
```
<br/>
  
  
## LOESS

Suppose a dataset consists of $N$ pairs of observations, $\{(x_i, y_i)\}_{i=1}^N$. For fitting a point $x$, define a bandwidth $h(x)$ and a smoothing window $(xâˆ’h(x),\,x+h(x))$. The LOWESS algorithm finds the vector $\boldsymbol\beta$ of coefficients which minimizes the locally weighted sum of squares
$$
\boldsymbol{\hat\beta} = \underset {\boldsymbol\beta \in \mathbb R^{p + 1}}{\arg\min} \sum_{i=1}^N w_i(x)(y_i - \langle{\boldsymbol\beta, \, A(x_i - x)}\rangle)^2
$$
where,  
$$
\begin{align}
A_0(v) = 1, &&
A_k(v) = \frac {v^k}{k!}, && 
w_i(x) = W\left(\frac {x_i - x}{h(x)}\right),
\end{align}
$$
$W(u)$ is a weight function that assigns largest weight to observations close to $x$.  

The local regression estimate is
$$
\hat f(x) = \langle \boldsymbol {\hat\beta}, \, A(0) \rangle = \boldsymbol {\hat\beta}_0.
$$


```{r loess}
set.seed(19)

period <- 120
x <- 1:period
y <- sin(2*pi*x/period) + runif(length(x),-1,1)

spanlist <- c(0.10, 0.25, 0.50, 0.75, 1.00, 2.00)
loess.lines <- sapply(spanlist, function(s) {
  y.loess <- loess(y ~ x, span = s, data.frame(x = x, y = y))
  cat("span = ", format(s, nsmall = 2), ", rse: ", y.loess$s, "\n", sep = "")
  predict(y.loess)
})
```
<br/>  
  
```{r loessplot, echo=FALSE, fig.width=7, fig.height=4}
colnames(loess.lines) <- paste("span =", spanlist)
loess.plt <- stack(as.data.frame(loess.lines))
loess.plt <- cbind(loess.plt, x = x)  # recycle x
ggplot(loess.plt, aes(x = x, y = values, group = ind, color = ind)) +
  geom_line(size = 0.5) +
  geom_point(aes(x, y), data = data.frame(x, y), inherit.aes = F) +
  scale_color_discrete("") +
  ggtitle("LOESS Smoothing") +
  theme_bw()
```
<br/>


## Logistic Regression

Formally, the logistic regression model is that
$$
\log {\frac {p(x)}{1 - p(x)}} = \beta_0 + \beta^T x.
$$
Suppose that we are given a sample $(x_i, y_i)$, $i = 1,\ldots,n$, where $y_i$ denotes the class $\in \{1, 2\}$ of the $i$-th observation. Then, assuming that the class labels are conditionally independent, the log-likelihood is given by
$$
\begin{aligned}
\ell(\beta_0, \beta) &= \sum_{i=1}^n \log {P(C = y_i \mid X = x_i)} &\\
                     &= \sum_{i=1}^n \left\{u_i \cdot (\beta_0 + \beta^T x_i) - \log {(1 + \exp (\beta_0 + \beta^T x_i))} \right\}
\end{aligned}
$$
where, 
$$
\begin{align}
u_i = \begin{cases} 1 & {\mbox{if }} y_i = 1 \\
                    0 & {\mbox{if }} y_i = 2 \end{cases} 
\end{align}
$$

The coefficients are estimated by maximizing the likelihood,
$$
\hat\beta_0, \hat\beta = \underset {\beta_0 \in \mathbb R, \beta \in \mathbb R^p}{\arg \max} \ell(\beta_0, \beta)
$$

```{r logreg}
mtcars.glm <- glm(vs ~ mpg, data = mtcars, family = binomial)
mtcars.glm
```
<br/>  
  
```{r logregplot, echo=FALSE, fig.width=5, fig.height=3.5}
ggplot(mtcars, aes(x = mpg, y = vs)) +
  theme_bw(base_size=16) +
  geom_point(alpha = 0.5, position = position_jitter(w = 0, h = 0.02)) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), 
              se = TRUE, colour = 'red', alpha = 0.3, fill = "grey") +
  labs(y = "Probability", x = "mpg")
```


---

In linear regression, the intercept coefficient is the expected value of the response when the predictor variables is zero, and the slopes represent the expected change in response for a unit change in the corresponding predictor. In the linear regression example, the average value of value of *mpg* is `r model$coef[1]` and for every 1 unit increase in *`r names(model$coef)[2]`*, *mpg* changes by `r model$coef[2]` units. Similarly, for every 1 unit increase in *`r names(model$coef)[3]`*, *mpg* changes by `r model$coef[3]` units.  

LOESS assumes no functional form and doesn't produce a regression function or slope parameters that is easily represented by a mathematical formula.  


The logistic slope can be interpreted as the effect of a unit change in predictor variable on the log odds ratio when the other variables in the model is held constant. The logistic intercept is the log odds estimate for the whole population. In the example for logistic regression, we can infer that the odds of having a V-engine, gets scaled by $\mathrm{e}^{`r mtcars.glm$coef[2]`} = `r exp(mtcars.glm$coef[2])`$ when *mpg* increases by one unit.

